{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mdxplain Tutorial: Introduction to Conformational Analysis with mdxplain\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Proteins are dynamic molecules that sample multiple conformational states during their lifespan. Understanding which conformations exist and what distinguishes them at the molecular level is crucial for comprehending protein mechanisms.\n",
    "The challenge in analyzing molecular dynamics (MD) trajectories lies in extracting biologically meaningful conformational states from thousands of simulation frames. Traditional approaches often require manual selection of collective variables or a priori knowledge of relevant structural features.\n",
    "\n",
    "This tutorial demonstrates mdxplain an automated pipeline builder that can be used to identify distinct conformational states and determines the specific residue-residue contacts that characterize each state. The approach combines machine learning techniques with structural biology principles to transform high-dimensional trajectory data into interpretable biological insights.\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "The pipeline addresses key challenges in conformational analysis:\n",
    "\n",
    "1. **Feature Engineering**: Convert atomic coordinates to rotation/translation-invariant contact descriptors\n",
    "2. **Dimensionality Reduction**: Overcome the curse of dimensionality for clustering in high-dimensional spaces\n",
    "3. **State Identification**: Determine the number and boundaries of conformational states\n",
    "4. **State Characterization**: Identify which molecular interactions distinguish each state\n",
    "\n",
    "### Test System\n",
    "\n",
    "We analyze the Villin headpiece (PDB: 2RJY [1]), a 64-residue protein domain that serves often as a model system for protein folding studies [2-7]. The trajectory contains 9,368 frames (9368 ns) with 1,027 atoms.\n",
    "\n",
    "The complete analysis requires 14 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "\n",
    "We just need to import one thing. Just the PipelineManager. The whole analysis can be managed from here. We do not need other stuff.\n",
    "\n",
    "## Design note\n",
    "\n",
    "Pipeline Builder Module\n",
    "- We use a builder pattern: the PipelineManager incrementally constructs an internal PipelineData structure.\n",
    "\n",
    "- Everything is centralized in `pipeline.data` and is enriched by the modules (trajectory, feature, feature_selector, decomposition, clustering, data_selector, comparison, feature_importance).\n",
    "\n",
    "Analysis module\n",
    "- The `analysis` module reads from `pipeline.data`.\n",
    "- It supports statistical analyses on features and structural analyses on trajectories such as RMSD and RMSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdxplain import PipelineManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Step 1: Data Loading and Preprocessing\n",
    "\n",
    "Efficient data handling is essential for processing large MD trajectories. Memory mapping allows analysis of datasets larger than available RAM by loading only required data sections. Residue labeling provides metadata necessary for interpreting structural features.\n",
    "\n",
    "Note on memory mapping (memmap) and chunk sizing\n",
    "- What memmap is: Arrays are kept on disk and only the required slices are loaded into RAM on demand. This keeps the memory footprint low at the cost of extra I/O.\n",
    "- When to enable memmap (use_memmap=True):\n",
    "  - Trajectories that approach/exceed available RAM\n",
    "  - Multiple trajectories opened at once\n",
    "  - Interactive analysis on laptops/workstations with limited memory\n",
    "- When to disable memmap (use_memmap=False):\n",
    "  - Small/medium datasets that comfortably fit in RAM\n",
    "  - You need maximum speed and have ample memory (e.g., HPC node)\n",
    "  - You are on a slow or remote filesystem where frequent random I/O is costly\n",
    "- Choosing chunk_size (number of frames processed per batch):\n",
    "  - Start with 500‚Äì2000 as a general rule of thumb; 1000 is a balanced default\n",
    "  - Increase if you have plenty of RAM (fewer I/O operations, faster overall)\n",
    "  - Decrease if you observe high memory usage, swapping, or OOM errors\n",
    "\n",
    "Quick mental model for memory needs\n",
    "- Per-batch memory = (n_features √ó bytes_per_feature) √ó chunk_size_frames + overhead\n",
    "- Examples: contacts often use 1 byte/feature; float32 distances use 4 bytes/feature\n",
    "- Aim for the batch to stay well below ~50% of available RAM to leave room for temporary arrays and the OS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using existing cache: ./cache/simulation_run1.dask.zarr\n",
      "\n",
      "Loaded 1 systems with 1 total trajectories:\n",
      "  2RJY: 1 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline and load trajectory data\n",
    "pipeline = PipelineManager(use_memmap=True, chunk_size=1000)\n",
    "pipeline.trajectory.load_trajectories(data_input=\"../data/2RJY/\")\n",
    "pipeline.trajectory.add_labels(traj_selection=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Feature Computation\n",
    "\n",
    "Raw atomic coordinates are sensitive to molecular rotation and translation, making direct comparison between frames problematic. Pairwise residue distances and contacts provide rotation/translation-invariant descriptors that capture the essential physics of protein structure. \n",
    "\n",
    "The distances default is a residue-residue metric. So all atom-atom distance between 2 residues are calculated and the closest heavy atom distance (without Hydrogen) is taken.\n",
    "\n",
    "The contacts used here in the \"contacts\" feature is a static boolean value. So if the residue-residue distance is lower the threshold, the contact-value is true, else it is false. It is kind of a representation, if two residues interact with each other or not.\n",
    "\n",
    "The 4.5 √Ö cutoff encompasses typical ranges for non-bonded interactions. It is a classical default, which is for example also used in mdciao or MDAnalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing distances:   0%|          | 0/1 [00:00<?, ?trajectories/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1953 residue pairs for 64 residues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing traj <DaskMDTrajectory with 9368 frames, 1027 atoms, 64 residues, and PBC (openmm format)>: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.10chunks/s]\n",
      "Converting units: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 826.56chunks/s]\n",
      "Computing distances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.12s/trajectories]\n",
      "Computing contacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 637.76chunks/s]\n",
      "Computing contacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.14trajectories/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reference\n",
    "# Compute distance and contact features\n",
    "pipeline.feature.add.distances()\n",
    "pipeline.feature.add.contacts(cutoff=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature Selection\n",
    "\n",
    "Why focus on contacts here\n",
    "- Interaction patterns can often reveal mechanistic differences between conformations more clearly than the magnitude of distance changes alone.\n",
    "- Contacts directly report formation/breaking of specific interactions, mapping naturally to hypotheses, mutational tests, and functional interpretation.\n",
    "- Therefore, in this tutorial we prioritize a contact-based analysis to highlight how interaction networks change across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created feature selector: 'contacts_only'\n",
      "Added to selector 'contacts_only': contacts -> 'all' (use_reduced=False, common_denominator=True, traj_selection=all, require_all_partners=False)\n",
      "Applied feature selector 'contacts_only' with reference trajectory 0 successfully\n"
     ]
    }
   ],
   "source": [
    "# Create feature selector focused on contact analysis\n",
    "# We create a selector and select the contacts of all available atoms / residues in this case\n",
    "# After this step, we call select to create the feature-matrix in our pipeline\n",
    "pipeline.feature_selector.create(\"contacts_only\")\n",
    "pipeline.feature_selector.add.contacts(\"contacts_only\", \"all\")\n",
    "pipeline.feature_selector.select(\"contacts_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Dimensionality Reduction\n",
    "\n",
    "The contact matrix contains (N¬≤ - N)/2 features (one per residue pair), creating a high-dimensional space where conventional clustering algorithms fail due to the \"curse of dimensionality.\" \n",
    "In high dimensions, all points become approximately equidistant, and noise dominates meaningful signals. Therefore we need to reduce the number of dimensions using a dimension reduction method.\n",
    "\n",
    "A classical way doing this is a PCA, which tries to neglect correlative information, i.e. redundancy and catch the axis with the most variant linear combinations of features.\n",
    "Cause MD data are often non-linear, a kernel PCA is the logical update to catch non-linear combinations of features.\n",
    "\n",
    "Cause we are doing a contact analysis, it could make sense to build a principle component space based directly on this data. Therefore we are doing a contact kernel PCA.\n",
    "\n",
    "This is basically a RBF kernel on the contact maps. This is equivalent to a hamming distance kernel. So we count the number of differences. \n",
    "\n",
    "So the first component holds the non-linear dynamic motion with the biggest change in the interaction pattern of the protein, the second the 2nd biggest and so on, which is a good space to cluster conformational states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating binary data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18296/18296 [00:00<00:00, 90540.32chunks/s]\n",
      "Computing kernel matrix rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.74chunks/s]\n",
      "Computing row sums: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 255.02chunks/s]\n",
      "Computing col sums: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 209.89chunks/s]\n",
      "Centering kernel matrix: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 24.83chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eigendecomposition for 10 components...\n",
      "  Matrix-vector products: 10\n",
      "  Matrix-vector products: 20\n",
      "  Matrix-vector products: 30\n",
      "  Matrix-vector products: 40\n",
      "Eigendecomposition completed after 40 matrix-vector products\n",
      "Decomposition 'contact_kernel_pca' with name 'ContactKernelPCA' for selection 'contacts_only' computed successfully. Data reduced from (9368, 1953) to (9368, 10).\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reference\n",
    "# Apply Contact Kernel PCA\n",
    "pipeline.decomposition.add.contact_kernel_pca(\n",
    "    n_components=10, \n",
    "    gamma=0.001, \n",
    "    selection_name=\"contacts_only\", \n",
    "    decomposition_name=\"ContactKernelPCA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Conformational State Identification\n",
    "\n",
    "The goal of this step is to group the simulation trajectory data into a set of distinct conformational states using a clustering algorithm.\n",
    "\n",
    "A primary challenge in this analysis is that conventional clustering methods, such as k-means, are problematic for this type of data. Such methods presuppose spherical cluster shapes and require the number of clusters to be known. Neither assumption is appropriate for the complex and unknown nature of conformational landscapes.\n",
    "\n",
    "To address these limitations, we employ a density based clustering in this example the Density Peak Advanced (DPA) algorithm. DPA identifies cluster centers as high-density regions that are separated by low-density boundaries, which allows it to automatically determine the optimal number of states. A Z-score(Z-value) threshold of 2.0 is used to achieve a balance between the sensitivity needed to resolve distinct conformational biases and the robustness required to filter out the thermal noise inherent to molecular dynamics simulations.\n",
    "\n",
    "In our daily analysis of MD data, we find that DPA is not only highly effective but also more straightforward to apply than other density-based methods, such as DBSCAN or HDBSCAN. Its primary advantage lies in its practical approach to hyperparameter tuning. While the DPA algorithm includes several configurable parameters, we have found that in practice, default values are typically sufficient for the clustering of conformational states. The Z-value can the be used to control the desired level of clustering granularity. Higher Z-values produce a coarser clustering with fewer, more distinct states, while lower values yield a more fine-grained result. This provides us with an intuitive mechanism to adjust the clustering resolution, circumventing the need to optimize the more abstract hyperparameters, such as eps and min_samples, required by (H)DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 'DPA' completed successfully.\n",
      "Found 6 clusters for 9368 frames.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reference\n",
    "\n",
    "# DPA clustering on decomposed data\n",
    "pipeline.clustering.add.dpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Comparative Analysis Setup\n",
    "\n",
    "Having identified conformational states, we now address the key biological question: which molecular interactions define each state? \n",
    "\n",
    "Data selectors organize trajectory frames by cluster membership, enabling systematic comparison between states. \n",
    "\n",
    "The one-vs-rest strategy identifies features that distinguish each state from all others, revealing the defining characteristics rather than just pairwise differences. \n",
    "\n",
    "This approach provides a comprehensive molecular fingerprint for each conformational state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data selectors for each cluster\n",
    "n_clusters = pipeline.data.cluster_data[\"DPA\"].get_n_clusters()\n",
    "for i in range(n_clusters):\n",
    "    pipeline.data_selector.create(f\"cluster_{i}\")\n",
    "    pipeline.data_selector.select_by_cluster(f\"cluster_{i}\", \"DPA\", [i])\n",
    "\n",
    "# Create one-vs-all comparison\n",
    "cluster_names = [f\"cluster_{i}\" for i in range(n_clusters)]\n",
    "pipeline.comparison.create_comparison(\n",
    "    name=\"cluster_comparison\",\n",
    "    mode=\"one_vs_rest\",\n",
    "    feature_selector=\"contacts_only\",\n",
    "    data_selectors=cluster_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: Identifying Key Contacts with a Decision Tree**\n",
    "\n",
    "So, after we set up our \"one-vs-rest\" comparisons in Step 6. We now have to ask the question of, how do we actually find the molecular interactions that define each state? For this, we will use decision trees.\n",
    "\n",
    "A decision tree is a great choice for this task because it creates a flowchart or tree of simple \"yes/no\" questions to classify the data. This is ideal for our purposes, cause tt works perfectly with our binary contact data (a contact is either formed or not) and is easy to interpret. It generates a set of human-readable rules that explain what makes a state unique.\n",
    "\n",
    "A crucial point is, that we are not trying to build a perfect model to predict the state of new, unseen data, but use it as an explanatory tool. \n",
    "Our goal is to have the model describe the data we already have as accurately as possible.\n",
    "\n",
    "We want it to tell us which contacts are the most important for distinguishing the conformational states we've found. Therefore, the traditional concern about \"overfitting\" doesn't apply here, cause we are interested in the features that define our specific dataset.\n",
    "\n",
    "To ensure the rules are simple and easy to understand, we limit the tree's complexity by setting `max_depth=3`. This forces the model to use only the most dominant and influential contacts to classify a state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 112.51chunks/s]\n",
      "\n",
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 115.01chunks/s]\n",
      "\n",
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 115.20chunks/s]\n",
      "\n",
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 115.35chunks/s]\n",
      "\n",
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 113.62chunks/s]\n",
      "\n",
      "Creating frame selection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 114.48chunks/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reference\n",
    "\n",
    "# Analyze feature importance\n",
    "pipeline.feature_importance.add.decision_tree(\n",
    "    comparison_name=\"cluster_comparison\",\n",
    "    analysis_name=\"feature_importance\",\n",
    "    max_depth=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8: Identifying the Top Contacts**\n",
    "\n",
    "The decision tree analysis provides a feature importance score for every contact. `scikit-learn` calculates this value as the Gini Importance or the Entropy,  depending on which `criterion` was used to create the trees. Default is `gini`.\n",
    "This score measures how effective a contact is at creating \"purer\" decision nodes in the tree. In other words, how well it helps to cleanly separate one state from all the others.\n",
    "\n",
    "These scores are normalized to a range between 0.0 and 1.0. A higher score means more important feature in the fingerprint of this cluster.\n",
    "\n",
    "To focus on the most essential information, we select the 3 contacts with the highest feature importance scores. These few contacts represent the core of the fingerprint for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 features distinguishing Cluster 0:\n",
      "  1. PRO17-SER43: 0.537\n",
      "  2. ALA26-GLY32: 0.379\n",
      "  3. PHE16-SER43: 0.071\n",
      "\n",
      "Top 3 features distinguishing Cluster 1:\n",
      "  1. ALA26-PRO35: 0.390\n",
      "  2. PRO17-SER43: 0.329\n",
      "  3. PRO35-LYS38: 0.096\n",
      "\n",
      "Top 3 features distinguishing Cluster 2:\n",
      "  1. LEU13-ARG31: 0.830\n",
      "  2. MET53-ARG55: 0.080\n",
      "  3. THR24-LEU29: 0.078\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reference\n",
    "\n",
    "# Get top features for each cluster vs rest\n",
    "for i in range(min(3, n_clusters)):  # Show first 3 clusters\n",
    "    sub_comparison_name = f\"cluster_{i}_vs_rest\"\n",
    "    \n",
    "    top_features = pipeline.feature_importance.get_top_features(\n",
    "        analysis_name=\"feature_importance\",\n",
    "        comparison_identifier=sub_comparison_name,\n",
    "        n=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop 3 features distinguishing Cluster {i}:\")\n",
    "    for j, feature_info in enumerate(top_features, 1):\n",
    "        print(f\"  {j}. {feature_info['feature_name']}: {feature_info['importance_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 9: Saving and loading the pipeline**\n",
    "\n",
    "In mdxplain you can save a whole pipeline with all your data and calculations and load it later again. Just do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save(\"cache/example_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipeline = PipelineManager()\n",
    "loaded_pipeline.load(\"cache/example_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 10: Info Printing**\n",
    "\n",
    "With `print_info()` you can get a lot of informations about the content of your pipeline. After or before the loading. To know, whats in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= PIPELINE INFORMATION =======\n",
      "\n",
      "--- Trajectory Data ---\n",
      "Loaded 1 trajectories:\n",
      "  [0] 2RJY_protein_protein_extended: 9368 frames\n",
      "\n",
      "--- Feature Data ---\n",
      "=== Feature Information ===\n",
      "Feature Types: 2 (distances, contacts)\n",
      "\n",
      "--- distances ---\n",
      "Trajectory 0:\n",
      "=== FeatureData ===\n",
      "Feature Type: Distances\n",
      "Original Data: 9368 frames x 1953 features\n",
      "Feature Metadata: 1953 feature definitions\n",
      "\n",
      "--- contacts ---\n",
      "Trajectory 0:\n",
      "=== FeatureData ===\n",
      "Feature Type: Contacts\n",
      "Original Data: 9368 frames x 1953 features\n",
      "Feature Metadata: 1953 feature definitions\n",
      "\n",
      "--- Feature Selection Data ---\n",
      "=== FeatureSelectorData Information ===\n",
      "FeatureSelectorData Names: 1 (contacts_only)\n",
      "\n",
      "--- contacts_only ---\n",
      "=== FeatureSelectorData ===\n",
      "Name: contacts_only\n",
      "Feature Types: 1 (contacts)\n",
      "Total Selections: 1\n",
      "Reference Trajectory: 0\n",
      "Matrix Columns: 1953\n",
      "Selection Results: Available for 1 feature types (contacts)\n",
      "\n",
      "--- Clustering Data ---\n",
      "=== ClusteringData Information ===\n",
      "ClusteringData Names: 1 (DPA)\n",
      "\n",
      "--- DPA ---\n",
      "=== ClusterData ===\n",
      "Cluster Type: DPA\n",
      "Number of Clusters: 6\n",
      "Number of Frames: 9368\n",
      "Hyperparameters: Z=3.0, metric=euclidean, affinity=precomputed, density_algo=PAk, k_max=1000, D_thr=23.92812698, dim_algo=twoNN, blockAn=True, block_ratio=20, frac=1.0, halos=False, method=standard, sample_size=9368, knn_neighbors=5, force=True\n",
      "Frame Mapping: 9368 frames from 1 trajectories\n",
      "\n",
      "--- Decomposition Data ---\n",
      "=== DecompositionData Information ===\n",
      "DecompositionData Names: 1 (ContactKernelPCA)\n",
      "\n",
      "--- ContactKernelPCA ---\n",
      "=== DecompositionData ===\n",
      "Decomposition Type: CONTACT_KERNEL_PCA\n",
      "Transformed Data: 9368 frames x 10 components\n",
      "Hyperparameters: n_components=10, kernel=rbf, gamma=0.001, use_nystrom=False, n_landmarks=2000, random_state=None, kernel_description=rbf on binary data (Hamming distance), contact_kernel=True, binary_data=True\n",
      "Frame Mapping: 9368 frames from 1 trajectories\n",
      "\n",
      "--- Data Selector Data ---\n",
      "=== DataSelectorData Information ===\n",
      "DataSelectorData Names: 6 (cluster_0, cluster_1, cluster_2, cluster_3, cluster_4, cluster_5)\n",
      "\n",
      "--- cluster_0 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_0\n",
      "Selected Frames: 279 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:1667-2452 (279)\n",
      "\n",
      "--- cluster_1 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_1\n",
      "Selected Frames: 819 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:0-4626 (819)\n",
      "\n",
      "--- cluster_2 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_2\n",
      "Selected Frames: 2 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:2817-3104 (2)\n",
      "\n",
      "--- cluster_3 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_3\n",
      "Selected Frames: 4901 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:3983-9367 (4901)\n",
      "\n",
      "--- cluster_4 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_4\n",
      "Selected Frames: 1038 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:1201-3888 (1038)\n",
      "\n",
      "--- cluster_5 ---\n",
      "=== DataSelectorData ===\n",
      "Name: cluster_5\n",
      "Selected Frames: 2329 frames from 1 trajectories\n",
      "Selection Type: cluster\n",
      "Frame Distribution: traj0:1-9364 (2329)\n",
      "\n",
      "--- Comparison Data ---\n",
      "=== Comparison Information ===\n",
      "Comparison Names: 1 (cluster_comparison)\n",
      "\n",
      "--- cluster_comparison ---\n",
      "=== ComparisonData ===\n",
      "Name: cluster_comparison\n",
      "Comparison Mode: one_vs_rest\n",
      "Feature Selector: contacts_only\n",
      "Data Selectors: 6 (cluster_0, cluster_1, cluster_2, cluster_3, cluster_4, cluster_5)\n",
      "Sub-Comparisons: 6 (cluster_0_vs_rest, cluster_1_vs_rest, cluster_2_vs_rest, cluster_3_vs_rest, cluster_4_vs_rest, cluster_5_vs_rest)\n",
      "\n",
      "--- Feature Importance Data ---\n",
      "=== FeatureImportanceData Information ===\n",
      "FeatureImportanceData Names: 1 (feature_importance)\n",
      "\n",
      "--- feature_importance ---\n",
      "=== FeatureImportanceData ===\n",
      "Name: feature_importance\n",
      "Analyzer Type: decision_tree\n",
      "Comparison: cluster_comparison\n",
      "Sub-Comparisons: 6 (cluster_0_vs_rest, cluster_1_vs_rest, cluster_2_vs_rest, cluster_3_vs_rest, cluster_4_vs_rest, cluster_5_vs_rest)\n",
      "Features Analyzed: 1953\n",
      "Top Feature Overall: Feature 266 (avg importance: 0.1979)\n",
      "Top 3 Features: Feature 266 (0.198), Feature 927 (0.189), Feature 679 (0.160)\n",
      "\n",
      "======= END PIPELINE INFORMATION =======\n",
      "\n",
      "Pipeline Summary: 1 trajectories, 2 feature types, 1 clusterings\n"
     ]
    }
   ],
   "source": [
    "loaded_pipeline.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This tutorial walked through an end‚Äëto‚Äëend, contact‚Äëfocused workflow: Feature engineering of residue contacts, dimensionality reduction with Contact Kernel PCA, and clustering with DPA, to identify conformational states and the specific interactions that define them. \n",
    "\n",
    "The key strength of mdxplain is its modular pipeline builder: each step can be swapped or extended without changing the overall flow, and all intermediate results are collected and reused via `pipeline.data`.\n",
    "\n",
    "Beyond the minimal example, mdxplain supports a broader feature set (e.g., DSSP/secondary structure, torsion angles, SASA, distances, raw coordinates) and multiple alternatives for dimensionality reduction and clustering. \n",
    "\n",
    "By standardizing data flow and bookkeeping, the library streamlines building and saving analyses. \n",
    "\n",
    "Common applications include:\n",
    "- Exporting feature matrices for downstream machine‚Äëlearning models.\n",
    "- Tag‚Äëbased comparisons across conditions (e.g., wild type vs mutant, ligand‚Äëbound vs apo) to identify discriminative features (contacts, secondary structure, SASA) and explain structural/dynamical effects.\n",
    "- Estimating state transition counts and preparing inputs for Markov state models.\n",
    "- Computing contact frequencies or other statistical feature properties and their differences across systems.\n",
    "- Doing RMSD and RMSF analysis for basic insights into the systems.\n",
    "- Paralell working on many trajectories at once.\n",
    "- Having many different metrices and features of many different trajectories at one place.\n",
    "\n",
    "And many more.\n",
    "\n",
    "To extend the workflow, `pipeline.analysis.structure` provides multiple RMSD/RMSF metrics, and `pipeline.analysis.feature.*` offers per‚Äëfeature statistics (e.g., contact frequencies, transition counts). Combining these with state discovery enables richer, multi‚Äëfaceted analyses that connect structure, dynamics, and feature statistics.\n",
    "\n",
    "### Limitations and outlook\n",
    "\n",
    "The quality of any analysis depends on adequate conformational sampling and the suitability of the chosen descriptors. Hyperparameters such as contact cutoffs, the number of components, and clustering sensitivity may require light tuning across systems.\n",
    "\n",
    "Also there are thousands of different other methods one could use for the analysis. But for this cases, mdxplain serves you with the necessary feature-data in an memory efficient, performant and streamlined way. \n",
    "\n",
    "But with this, there is another limitation. Disk space. If you use your disk for big analysis, cause your trajectory and feature data does not fit in RAM, you need disk space. For hugh system this can also become a problem. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "mdxplain lowers the effort to move from raw trajectories to interpretable, testable insights by simplifying the composition, execution, and persistence of complex structural analysis pipelines. Its modular design and extensibility make it a practical tool across structural biology use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Crystal Structure of a pH-Stabilized Mutant of Villin Headpiece, Jianmin Meng and C. James McKnight, Biochemistry 2008 47 (16), 4644-4650, DOI: 10.1021/bi7022738\n",
    "\n",
    "[2] ROCKLIN, Gabriel J., et al. Global analysis of protein folding using massively parallel design, synthesis, and testing. Science, 2017, 357. Jg., Nr. 6347, S. 168-175.\n",
    "\n",
    "[3] Melvin RL, Godwin RC, Xiao J, Thompson WG, Berenhaut KS, Salsbury FR Jr. Uncovering Large-Scale Conformational Change in Molecular Dynamics without Prior Knowledge. J Chem Theory Comput. 2016 Dec 13;12(12):6130-6146. doi: 10.1021/acs.jctc.6b00757\n",
    "\n",
    "[4] H. Lei,C. Wu,H. Liu, & Y. Duan,  Folding free-energy landscape of villin headpiece subdomain from molecular dynamics simulations, Proc. Natl. Acad. Sci. U.S.A. 104 (12) 4925-4930, https://doi.org/10.1073/pnas.0608432104 (2007).\n",
    "\n",
    "[5] Ensign DL, Kasson PM, Pande VS. Heterogeneity even at the speed limit of folding: large-scale molecular dynamics study of a fast-folding variant of the villin headpiece. J Mol Biol. 2007 Nov 30;374(3):806-16. doi: 10.1016/j.jmb.2007.09.069. Epub 2007 Sep 29. Erratum in: J Mol Biol. 2008 Nov 21;383(4):935. PMID: 17950314; PMCID: PMC3689540.\n",
    "\n",
    "[6] Roy Gonz√°lez-Alem√°n, Daniel Platero-Rochart, David Hern√°ndez-Castillo, Erix W Hern√°ndez-Rodr√≠guez, Julio Caballero, Fabrice Leclerc, Luis Montero-Cabrera, BitQT: a graph-based approach to the quality threshold clustering of molecular dynamics, Bioinformatics, Volume 38, Issue 1, January 2022, Pages 73‚Äì79, https://doi.org/10.1093/bioinformatics/btab595\n",
    "\n",
    "[7] WANG, Ercheng, et al. A novel folding pathway of the villin headpiece subdomain HP35. Physical Chemistry Chemical Physics, 2019, 21. Jg., Nr. 33, S. 18219-18226."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Disclaimer\n",
    "\n",
    "This notebook was written with the suppport of Github Copilot with OpenAI GPT-5 and Google Gemini 2.5 Pro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdxplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
